from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
import time
import random

# Function to introduce human-like delays
def human_delay(min_time=2, max_time=5):
    time.sleep(random.uniform(min_time, max_time))

# Set up Edge WebDriver with anti-detection settings
options = webdriver.EdgeOptions()
options.add_argument("--start-maximized")  # Maximize window
options.add_argument("--disable-blink-features=AutomationControlled")  # Remove Selenium detection
options.add_experimental_option("excludeSwitches", ["enable-automation"])  # Remove automation flag
options.add_experimental_option("useAutomationExtension", False)  # Disable automation extension

# Set a random user-agent
user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:112.0) Gecko/20100101 Firefox/112.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
]
random_user_agent = random.choice(user_agents)
options.add_argument(f"user-agent={random_user_agent}")  # Set a random user agent

# Launch the browser
driver = webdriver.Edge(options=options)

# Remove "navigator.webdriver" detection
driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

try:
    # Step 1: Open the submissions page
    driver.get("https://app.any.run/submissions")
    human_delay()  # Introduce a delay

    # Step 2: Locate search bar and search for "telegram"
    search_box = driver.find_element(By.ID, "historySearch")
    search_box.send_keys("telegram")
    human_delay(1, 3)  # Randomized delay before hitting Enter
    search_box.send_keys(Keys.RETURN)
    human_delay(3, 6)  # Wait for results to load

    # Step 3: Extract first 10 rows
    rows = driver.find_elements(By.CLASS_NAME, "history-table--content__row")[:20]
    extracted_urls = []

    for row in rows:
        # Extract task URL
        task_link = row.find_element(By.TAG_NAME, "a").get_attribute("href")
        task_id = task_link.split("/")[-1]  # Extract the last part of URL

        # Extract SHA256 hash and convert to lowercase
        sha256_element = row.find_elements(By.XPATH, ".//div[contains(@class,'hash__item') and div[text()='sha256:']]//div[@class='hash__value']")
        if sha256_element:
            sha256_hash = sha256_element[0].text.strip().lower()  # Convert to lowercase
            extracted_urls.append(f"https://any.run/report/{sha256_hash}/{task_id}")

    # Step 4: Save results to file
    with open("urls_telegram.txt", "w") as file:
        for url in extracted_urls:
            file.write(url + "\n")

    print("âœ… Scraping complete. Results saved in urls_telegram.txt.")

finally:
    driver.quit()  # Close browser
